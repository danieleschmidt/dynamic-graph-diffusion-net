#!/usr/bin/env python3
"""
Generation 2 Completion Report: Comprehensive Robustness Validation
Final validation of all Generation 2 robustness enhancements.
"""

import torch
import time
import json
import os
import sys
from typing import Dict, Any, List
from dataclasses import asdict

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# Import Generation 2 components
from gen2_robustness_suite import RobustDGDNWrapper, PerformanceMonitor, SecurityManager, InputValidator
from gen2_fault_tolerance import FaultTolerantDGDN, CircuitBreaker, CircuitBreakerConfig
from gen2_config_management import ConfigManager, Environment, DGDNConfig

def run_comprehensive_robustness_test():
    """Run comprehensive robustness validation test."""
    print("üõ°Ô∏è GENERATION 2: COMPREHENSIVE ROBUSTNESS VALIDATION")
    print("=" * 60)
    
    test_results = {
        'timestamp': time.time(),
        'generation': 2,
        'component_tests': {},
        'integration_tests': {},
        'performance_metrics': {},
        'security_validations': {},
        'overall_status': 'UNKNOWN'
    }
    
    try:
        # 1. Test Core Robustness Components
        print("\nüîß Testing Core Robustness Components...")
        
        # Input validation
        print("   üîç Input Validation...")
        validator = InputValidator()
        
        # Create test data
        class TestData:
            def __init__(self):
                self.edge_index = torch.randint(0, 100, (2, 200))
                self.timestamps = torch.sort(torch.rand(200) * 100.0)[0]
                self.node_features = torch.randn(100, 64)
                self.num_nodes = 100
        
        test_data = TestData()
        validation_result = validator.validate_temporal_data(test_data)
        
        test_results['component_tests']['input_validation'] = {
            'status': 'PASS' if validation_result.is_valid else 'FAIL',
            'errors': validation_result.errors,
            'warnings': validation_result.warnings,
            'metrics': validation_result.metrics
        }
        print(f"   ‚úÖ Input Validation: {'PASS' if validation_result.is_valid else 'FAIL'}")
        
        # Security manager
        print("   üîí Security Manager...")
        security = SecurityManager()
        sanitized = security.sanitize_input_data(test_data)
        
        test_results['component_tests']['security_manager'] = {
            'status': 'PASS' if sanitized else 'FAIL',
            'sanitization_passed': sanitized
        }
        print(f"   ‚úÖ Security Manager: {'PASS' if sanitized else 'FAIL'}")
        
        # Performance monitor
        print("   üìä Performance Monitor...")
        monitor = PerformanceMonitor()
        
        with monitor.monitor_inference():
            time.sleep(0.01)  # Simulate inference
        
        metrics = monitor.get_average_metrics(1)
        test_results['component_tests']['performance_monitor'] = {
            'status': 'PASS',
            'metrics': asdict(metrics)
        }
        print(f"   ‚úÖ Performance Monitor: PASS")
        
        # 2. Test Robust Model Wrapper
        print("\nüöÄ Testing Robust Model Wrapper...")
        
        config = {
            'node_dim': 64,
            'hidden_dim': 128,
            'time_dim': 32,
            'num_layers': 2,
            'num_heads': 4,
            'diffusion_steps': 3,
            'dropout': 0.1
        }
        
        robust_model = RobustDGDNWrapper(config)
        
        # Test normal operation
        try:
            output = robust_model.forward(test_data)
            health = robust_model.get_health_status()
            
            test_results['component_tests']['robust_wrapper'] = {
                'status': 'PASS',
                'forward_pass': True,
                'health_check': health['model_status'] == 'healthy',
                'output_keys': list(output.keys())
            }
            print(f"   ‚úÖ Robust Model Wrapper: PASS")
            
        except Exception as e:
            test_results['component_tests']['robust_wrapper'] = {
                'status': 'FAIL',
                'error': str(e)
            }
            print(f"   ‚ùå Robust Model Wrapper: FAIL - {str(e)}")
        
        # 3. Test Fault Tolerance
        print("\n‚ö° Testing Fault Tolerance...")
        
        try:
            ft_model = FaultTolerantDGDN(config)
            
            # Test normal operation
            output = ft_model.forward(test_data)
            
            # Test checkpoint
            checkpoint_path = ft_model.save_checkpoint()
            
            # Test circuit breaker (force failure)
            failures = 0
            try:
                class BadData:
                    pass
                
                for _ in range(5):
                    try:
                        ft_model.forward(BadData(), use_fallback=False)
                    except:
                        failures += 1
            except:
                pass
            
            system_health = ft_model.get_system_health()
            
            test_results['component_tests']['fault_tolerance'] = {
                'status': 'PASS',
                'normal_operation': True,
                'checkpoint_saved': os.path.exists(checkpoint_path),
                'circuit_breaker_triggered': failures >= 3,
                'system_health': system_health['model_status']
            }
            print(f"   ‚úÖ Fault Tolerance: PASS")
            
        except Exception as e:
            test_results['component_tests']['fault_tolerance'] = {
                'status': 'FAIL',
                'error': str(e)
            }
            print(f"   ‚ùå Fault Tolerance: FAIL - {str(e)}")
        
        # 4. Test Configuration Management
        print("\n‚öôÔ∏è Testing Configuration Management...")
        
        try:
            config_manager = ConfigManager("test_configs_gen2")
            
            # Create and load configs
            config_manager.create_default_configs()
            dev_config = config_manager.load_config(Environment.DEVELOPMENT)
            prod_config = config_manager.load_config(Environment.PRODUCTION)
            
            # Test overrides
            overrides = {'model.hidden_dim': 512}
            override_config = config_manager.override_config(overrides)
            
            test_results['component_tests']['config_management'] = {
                'status': 'PASS',
                'default_configs_created': True,
                'dev_config_loaded': dev_config.deployment.environment == Environment.DEVELOPMENT,
                'prod_config_loaded': prod_config.deployment.environment == Environment.PRODUCTION,
                'overrides_applied': override_config.model.hidden_dim == 512,
                'current_env': config_manager.get_current_environment().value
            }
            print(f"   ‚úÖ Configuration Management: PASS")
            
        except Exception as e:
            test_results['component_tests']['config_management'] = {
                'status': 'FAIL',
                'error': str(e)
            }
            print(f"   ‚ùå Configuration Management: FAIL - {str(e)}")
        
        # 5. Integration Tests
        print("\nüîó Running Integration Tests...")
        
        # Test end-to-end robustness
        try:
            # Load production config
            config_manager = ConfigManager("test_configs_gen2")
            prod_config = config_manager.load_config(Environment.PRODUCTION)
            
            # Create robust model with production config
            model_config = asdict(prod_config.model)
            robust_model = RobustDGDNWrapper(model_config)
            
            # Test full pipeline
            start_time = time.time()
            
            # Multiple inferences to test stability
            for i in range(10):
                output = robust_model.forward(test_data)
                
                # Test edge prediction
                src_nodes = torch.randint(0, 100, (5,))
                tgt_nodes = torch.randint(0, 100, (5,))
                predictions = robust_model.predict_edges(src_nodes, tgt_nodes, 50.0, test_data)
            
            integration_time = time.time() - start_time
            final_health = robust_model.get_health_status()
            
            test_results['integration_tests']['end_to_end'] = {
                'status': 'PASS',
                'inference_count': 10,
                'total_time_seconds': integration_time,
                'avg_time_per_inference': integration_time / 10,
                'final_health': final_health['model_status'],
                'memory_usage_mb': final_health['performance_metrics']['memory_usage_mb']
            }
            print(f"   ‚úÖ End-to-End Integration: PASS")
            print(f"      - 10 inferences in {integration_time:.2f}s")
            print(f"      - Avg time per inference: {integration_time/10*1000:.1f}ms")
            
        except Exception as e:
            test_results['integration_tests']['end_to_end'] = {
                'status': 'FAIL',
                'error': str(e)
            }
            print(f"   ‚ùå End-to-End Integration: FAIL - {str(e)}")
        
        # 6. Security Validations
        print("\nüîí Running Security Validations...")
        
        security_tests = {
            'input_sanitization': False,
            'bounds_checking': False,
            'model_integrity': False,
            'error_handling': False
        }
        
        try:
            # Test input sanitization
            malicious_data = TestData()
            malicious_data.edge_index = torch.tensor([[999999], [-1]])  # Out of bounds
            malicious_data.num_nodes = 100
            
            try:
                robust_model.forward(malicious_data)
                security_tests['input_sanitization'] = False
            except ValueError:
                security_tests['input_sanitization'] = True  # Should catch malicious input
            
            # Test bounds checking
            try:
                invalid_nodes = torch.tensor([1000])  # Out of bounds
                robust_model.predict_edges(invalid_nodes, invalid_nodes, 50.0, test_data)
                security_tests['bounds_checking'] = False
            except ValueError:
                security_tests['bounds_checking'] = True  # Should catch out of bounds
            
            # Test model integrity
            initial_hash = security.hash_model_state(robust_model.model)
            integrity_check = security.validate_model_integrity(robust_model.model, initial_hash)
            security_tests['model_integrity'] = integrity_check
            
            # Test error handling
            try:
                class InvalidTestData:
                    pass
                robust_model.forward(InvalidTestData())
                security_tests['error_handling'] = False
            except (ValueError, AttributeError):
                security_tests['error_handling'] = True  # Should handle gracefully
            
            test_results['security_validations'] = security_tests
            security_pass = all(security_tests.values())
            print(f"   {'‚úÖ' if security_pass else '‚ùå'} Security Validations: {'PASS' if security_pass else 'FAIL'}")
            
        except Exception as e:
            test_results['security_validations'] = {
                'status': 'FAIL',
                'error': str(e)
            }
            print(f"   ‚ùå Security Validations: FAIL - {str(e)}")
        
        # 7. Performance Benchmarks
        print("\nüìä Running Performance Benchmarks...")
        
        try:
            # Benchmark different model sizes
            benchmark_configs = [
                {'hidden_dim': 64, 'num_layers': 1, 'name': 'small'},
                {'hidden_dim': 128, 'num_layers': 2, 'name': 'medium'},
                {'hidden_dim': 256, 'num_layers': 3, 'name': 'large'}
            ]
            
            benchmark_results = {}
            
            for bench_config in benchmark_configs:
                config_copy = config.copy()
                config_copy.update({k: v for k, v in bench_config.items() if k != 'name'})
                
                bench_model = RobustDGDNWrapper(config_copy)
                
                # Warm up
                bench_model.forward(test_data)
                
                # Benchmark
                start_time = time.time()
                for _ in range(5):
                    bench_model.forward(test_data)
                bench_time = time.time() - start_time
                
                health = bench_model.get_health_status()
                
                benchmark_results[bench_config['name']] = {
                    'avg_inference_time_ms': (bench_time / 5) * 1000,
                    'memory_usage_mb': health['performance_metrics']['memory_usage_mb'],
                    'hidden_dim': config_copy['hidden_dim'],
                    'num_layers': config_copy['num_layers']
                }
            
            test_results['performance_metrics'] = benchmark_results
            print(f"   ‚úÖ Performance Benchmarks: PASS")
            for name, metrics in benchmark_results.items():
                print(f"      - {name}: {metrics['avg_inference_time_ms']:.1f}ms, {metrics['memory_usage_mb']:.1f}MB")
            
        except Exception as e:
            test_results['performance_metrics'] = {
                'status': 'FAIL',
                'error': str(e)
            }
            print(f"   ‚ùå Performance Benchmarks: FAIL - {str(e)}")
        
        # Calculate overall status
        component_passes = sum(1 for test in test_results['component_tests'].values() 
                             if isinstance(test, dict) and test.get('status') == 'PASS')
        total_components = len(test_results['component_tests'])
        
        integration_passes = sum(1 for test in test_results['integration_tests'].values()
                               if isinstance(test, dict) and test.get('status') == 'PASS')
        total_integrations = len(test_results['integration_tests'])
        
        security_passes = sum(1 for passed in test_results['security_validations'].values()
                            if passed is True)
        total_security = len([v for v in test_results['security_validations'].values() if isinstance(v, bool)])
        
        performance_passes = 1 if 'status' not in test_results['performance_metrics'] else 0
        total_performance = 1
        
        total_passes = component_passes + integration_passes + security_passes + performance_passes
        total_tests = total_components + total_integrations + total_security + total_performance
        
        success_rate = total_passes / total_tests if total_tests > 0 else 0
        
        if success_rate >= 0.9:
            test_results['overall_status'] = 'EXCELLENT'
        elif success_rate >= 0.8:
            test_results['overall_status'] = 'GOOD'
        elif success_rate >= 0.7:
            test_results['overall_status'] = 'ACCEPTABLE'
        else:
            test_results['overall_status'] = 'NEEDS_IMPROVEMENT'
        
        # Final Report
        print(f"\n{'='*60}")
        print("üéØ GENERATION 2 ROBUSTNESS VALIDATION REPORT")
        print(f"{'='*60}")
        print(f"Overall Status: {test_results['overall_status']}")
        print(f"Success Rate: {success_rate*100:.1f}% ({total_passes}/{total_tests})")
        print(f"\nComponent Tests: {component_passes}/{total_components} passed")
        print(f"Integration Tests: {integration_passes}/{total_integrations} passed")
        print(f"Security Validations: {security_passes}/{total_security} passed")
        print(f"Performance Benchmarks: {performance_passes}/{total_performance} passed")
        
        print(f"\nüìä Performance Summary:")
        if 'small' in test_results.get('performance_metrics', {}):
            for size, metrics in test_results['performance_metrics'].items():
                if isinstance(metrics, dict):
                    print(f"   {size.capitalize()}: {metrics['avg_inference_time_ms']:.1f}ms")
        
        print(f"\nüõ°Ô∏è Security Status:")
        if isinstance(test_results['security_validations'], dict):
            for test_name, passed in test_results['security_validations'].items():
                if isinstance(passed, bool):
                    status = "‚úÖ PASS" if passed else "‚ùå FAIL"
                    print(f"   {test_name.replace('_', ' ').title()}: {status}")
        
        # Save detailed report
        report_path = "gen2_robustness_report.json"
        with open(report_path, 'w') as f:
            json.dump(test_results, f, indent=2, default=str)
        
        print(f"\nüìÑ Detailed report saved: {report_path}")
        
        if success_rate >= 0.85:
            print(f"\nüéâ GENERATION 2 ROBUSTNESS: VALIDATION SUCCESSFUL!")
            print(f"‚úÖ Ready to proceed to Generation 3 (Optimization & Scaling)")
            return True
        else:
            print(f"\n‚ö†Ô∏è GENERATION 2 ROBUSTNESS: NEEDS IMPROVEMENT")
            print(f"‚ùå Address failing tests before proceeding to Generation 3")
            return False
        
    except Exception as e:
        print(f"\n‚ùå CRITICAL ERROR in Generation 2 validation: {str(e)}")
        import traceback
        traceback.print_exc()
        test_results['overall_status'] = 'CRITICAL_FAILURE'
        test_results['error'] = str(e)
        return False

if __name__ == "__main__":
    success = run_comprehensive_robustness_test()
    sys.exit(0 if success else 1)