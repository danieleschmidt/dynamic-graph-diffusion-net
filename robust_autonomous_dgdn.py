#!/usr/bin/env python3
"""Robust Autonomous DGDN - Generation 2 Implementation.

Enhanced with comprehensive error handling, validation, logging, monitoring,
and security measures following Terragon SDLC methodology.
"""

import sys
import time
import math
import random
import logging
import traceback
import hashlib
import json
import os
from typing import Dict, Any, List, Tuple, Optional, Union
from dataclasses import dataclass
from contextlib import contextmanager
from enum import Enum

# Enhanced logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('dgdn_robust.log', mode='a')
    ]
)
logger = logging.getLogger(__name__)

# Try to import enhanced dependencies
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

class ValidationError(Exception):
    """Custom validation error."""
    pass

class SecurityError(Exception):
    """Custom security error."""
    pass

class PerformanceError(Exception):
    """Custom performance error.""" 
    pass

class DGDNStatus(Enum):
    """DGDN execution status codes."""
    INITIALIZING = "initializing"
    READY = "ready"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    DEGRADED = "degraded"

@dataclass
class SecurityConfig:
    """Security configuration parameters."""
    max_nodes: int = 10000
    max_edges: int = 100000
    max_memory_mb: int = 1024
    enable_input_sanitization: bool = True
    enable_output_validation: bool = True
    hash_validation: bool = True

@dataclass
class PerformanceConfig:
    """Performance monitoring configuration."""
    max_inference_time_ms: int = 5000
    memory_warning_threshold_mb: int = 512
    enable_profiling: bool = True
    enable_caching: bool = True

@dataclass
class RobustnessMetrics:
    """Comprehensive robustness metrics."""
    validation_errors: int = 0
    security_warnings: int = 0
    performance_warnings: int = 0
    auto_corrections: int = 0
    circuit_breaker_trips: int = 0
    cache_hits: int = 0
    cache_misses: int = 0

class InputValidator:
    """Comprehensive input validation and sanitization."""
    
    def __init__(self, security_config: SecurityConfig):
        self.security_config = security_config
        self.logger = logging.getLogger(f"{__name__}.InputValidator")
    
    def validate_graph_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and sanitize graph data."""
        self.logger.info("üîç Validating graph data input")
        
        try:
            # Check required keys
            required_keys = ['node_features', 'edges', 'num_nodes', 'num_edges']
            for key in required_keys:
                if key not in data:
                    raise ValidationError(f"Missing required key: {key}")
            
            # Validate dimensions
            num_nodes = data['num_nodes']
            num_edges = data['num_edges']
            
            if num_nodes <= 0:
                raise ValidationError(f"Invalid number of nodes: {num_nodes}")
            if num_edges < 0:
                raise ValidationError(f"Invalid number of edges: {num_edges}")
            
            # Security checks
            if num_nodes > self.security_config.max_nodes:
                raise SecurityError(f"Nodes {num_nodes} exceeds limit {self.security_config.max_nodes}")
            if num_edges > self.security_config.max_edges:
                raise SecurityError(f"Edges {num_edges} exceeds limit {self.security_config.max_edges}")
            
            # Validate node features
            node_features = data['node_features']
            if len(node_features) != num_nodes:
                raise ValidationError(f"Node features length {len(node_features)} != num_nodes {num_nodes}")
            
            # Check for malicious patterns in features
            if self.security_config.enable_input_sanitization:
                node_features = self._sanitize_features(node_features)
                data['node_features'] = node_features
            
            # Validate edges
            edges = data['edges']
            if len(edges) != num_edges:
                self.logger.warning(f"Edge count mismatch: {len(edges)} vs {num_edges}, correcting...")
                data['num_edges'] = len(edges)
            
            # Validate edge structure
            for i, edge in enumerate(edges):
                if len(edge) != 4:  # (source, target, timestamp, weight)
                    raise ValidationError(f"Edge {i} has invalid structure: {len(edge)} elements")
                
                source, target, timestamp, weight = edge
                
                # Validate indices
                if not (0 <= source < num_nodes):
                    raise ValidationError(f"Edge {i} source {source} out of range [0, {num_nodes})")
                if not (0 <= target < num_nodes):
                    raise ValidationError(f"Edge {i} target {target} out of range [0, {num_nodes})")
                
                # Validate numeric values
                if not isinstance(timestamp, (int, float)) or not math.isfinite(timestamp):
                    raise ValidationError(f"Edge {i} invalid timestamp: {timestamp}")
                if not isinstance(weight, (int, float)) or not math.isfinite(weight):
                    raise ValidationError(f"Edge {i} invalid weight: {weight}")
                
                # Security: Check for extreme values
                if abs(timestamp) > 1e6:
                    self.logger.warning(f"Edge {i} has extreme timestamp: {timestamp}")
                if abs(weight) > 1e3:
                    self.logger.warning(f"Edge {i} has extreme weight: {weight}")
            
            self.logger.info(f"‚úÖ Graph data validated: {num_nodes} nodes, {len(edges)} edges")
            return data
            
        except Exception as e:
            self.logger.error(f"‚ùå Validation failed: {e}")
            raise
    
    def _sanitize_features(self, features: List[List[float]]) -> List[List[float]]:
        """Sanitize feature values to prevent attacks."""
        sanitized = []
        
        for node_idx, node_features in enumerate(features):
            sanitized_node = []
            
            for feat_idx, value in enumerate(node_features):
                # Check for NaN/Inf
                if not math.isfinite(value):
                    self.logger.warning(f"Sanitizing non-finite value at node {node_idx}, feature {feat_idx}")
                    value = 0.0
                
                # Clamp extreme values
                if abs(value) > 100.0:
                    self.logger.warning(f"Clamping extreme value {value} at node {node_idx}")
                    value = math.copysign(min(abs(value), 100.0), value)
                
                sanitized_node.append(value)
            
            sanitized.append(sanitized_node)
        
        return sanitized
    
    def compute_data_hash(self, data: Dict[str, Any]) -> str:
        """Compute security hash of input data."""
        try:
            # Create deterministic string representation
            data_str = json.dumps({
                'num_nodes': data['num_nodes'],
                'num_edges': data['num_edges'],
                'features_shape': [len(data['node_features']), len(data['node_features'][0])],
                'edges_sample': data['edges'][:min(10, len(data['edges']))]  # First 10 edges
            }, sort_keys=True)
            
            return hashlib.sha256(data_str.encode()).hexdigest()[:16]
        
        except Exception as e:
            self.logger.warning(f"Could not compute data hash: {e}")
            return "unknown"

class CircuitBreaker:
    """Circuit breaker pattern for fault tolerance."""
    
    def __init__(self, failure_threshold: int = 3, recovery_timeout: float = 30.0):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = 0
        self.state = "closed"  # closed, open, half_open
        self.logger = logging.getLogger(f"{__name__}.CircuitBreaker")
    
    def call(self, func, *args, **kwargs):
        """Execute function with circuit breaker protection."""
        if self.state == "open":
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = "half_open"
                self.logger.info("üîÑ Circuit breaker entering half-open state")
            else:
                raise PerformanceError("Circuit breaker is open - service temporarily unavailable")
        
        try:
            result = func(*args, **kwargs)
            
            if self.state == "half_open":
                self.state = "closed"
                self.failure_count = 0
                self.logger.info("‚úÖ Circuit breaker closed - service recovered")
            
            return result
            
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.failure_count >= self.failure_threshold:
                self.state = "open"
                self.logger.error(f"üö® Circuit breaker opened after {self.failure_count} failures")
            
            raise

class PerformanceMonitor:
    """Enhanced performance monitoring and optimization."""
    
    def __init__(self, config: PerformanceConfig):
        self.config = config
        self.metrics = {}
        self.cache = {} if config.enable_caching else None
        self.logger = logging.getLogger(f"{__name__}.PerformanceMonitor")
    
    @contextmanager
    def monitor_operation(self, operation_name: str):
        """Context manager for monitoring operations."""
        start_time = time.time()
        start_memory = self._get_memory_usage()
        
        self.logger.info(f"üìä Starting operation: {operation_name}")
        
        try:
            yield
            
            # Success metrics
            duration = time.time() - start_time
            memory_used = self._get_memory_usage() - start_memory
            
            self.metrics[operation_name] = {
                'duration': duration,
                'memory_used': memory_used,
                'status': 'success',
                'timestamp': time.time()
            }
            
            # Performance warnings
            if duration * 1000 > self.config.max_inference_time_ms:
                self.logger.warning(f"‚ö†Ô∏è  Operation {operation_name} took {duration:.3f}s (limit: {self.config.max_inference_time_ms}ms)")
            
            if memory_used > self.config.memory_warning_threshold_mb:
                self.logger.warning(f"‚ö†Ô∏è  Operation {operation_name} used {memory_used}MB memory")
            
            self.logger.info(f"‚úÖ Operation {operation_name} completed in {duration:.3f}s")
            
        except Exception as e:
            duration = time.time() - start_time
            self.metrics[operation_name] = {
                'duration': duration,
                'status': 'failed',
                'error': str(e),
                'timestamp': time.time()
            }
            self.logger.error(f"‚ùå Operation {operation_name} failed after {duration:.3f}s: {e}")
            raise
    
    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB."""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0
    
    def get_cache_key(self, *args) -> str:
        """Generate cache key from arguments."""
        try:
            return hashlib.md5(str(args).encode()).hexdigest()
        except:
            return str(hash(args))
    
    def cache_get(self, key: str) -> Any:
        """Get value from cache."""
        if self.cache is None:
            return None
        return self.cache.get(key)
    
    def cache_set(self, key: str, value: Any) -> None:
        """Set value in cache."""
        if self.cache is None:
            return
        
        # Simple cache size limit
        if len(self.cache) > 100:
            # Remove oldest entry
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        self.cache[key] = value

class RobustDGDN:
    """Robust DGDN with comprehensive error handling and validation."""
    
    def __init__(self, 
                 node_dim: int = 32, 
                 hidden_dim: int = 64, 
                 num_layers: int = 2,
                 security_config: Optional[SecurityConfig] = None,
                 performance_config: Optional[PerformanceConfig] = None):
        
        self.node_dim = node_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.status = DGDNStatus.INITIALIZING
        
        # Configuration
        self.security_config = security_config or SecurityConfig()
        self.performance_config = performance_config or PerformanceConfig()
        
        # Components
        self.validator = InputValidator(self.security_config)
        self.circuit_breaker = CircuitBreaker()
        self.performance_monitor = PerformanceMonitor(self.performance_config)
        self.metrics = RobustnessMetrics()
        
        # Initialize logger first
        self.logger = logging.getLogger(f"{__name__}.RobustDGDN")
        
        # Model parameters
        self.weights = self._safe_initialize_weights()
        self.adaptive_params = {
            'message_strength': 1.0,
            'temporal_weight': 0.5,
            'uncertainty_scale': 0.1,
            'robustness_factor': 1.0
        }
        
        self.status = DGDNStatus.READY
        
        self.logger.info(f"üõ°Ô∏è  Robust DGDN initialized: {node_dim}‚Üí{hidden_dim}, {num_layers} layers")
    
    def _safe_initialize_weights(self) -> Dict[str, Any]:
        """Safely initialize weights with validation."""
        try:
            random.seed(42)
            
            weights = {}
            
            # Validate dimensions
            if self.node_dim <= 0 or self.hidden_dim <= 0:
                raise ValueError(f"Invalid dimensions: {self.node_dim}, {self.hidden_dim}")
            
            # Initialize with He initialization for better gradient flow
            node_std = math.sqrt(2.0 / self.node_dim)
            temporal_std = math.sqrt(2.0 / 32)
            hidden_std = math.sqrt(2.0 / (self.hidden_dim * 2))
            
            weights['node_projection'] = [
                [random.gauss(0, node_std) for _ in range(self.hidden_dim)]
                for _ in range(self.node_dim)
            ]
            
            weights['temporal_projection'] = [
                [random.gauss(0, temporal_std) for _ in range(self.hidden_dim)]
                for _ in range(32)
            ]
            
            weights['message_weights'] = [
                [random.gauss(0, hidden_std) for _ in range(self.hidden_dim)]
                for _ in range(self.hidden_dim * 2)
            ]
            
            # Validate weight initialization
            for weight_name, weight_matrix in weights.items():
                if not weight_matrix or not weight_matrix[0]:
                    raise ValueError(f"Invalid weight matrix: {weight_name}")
                
                # Check for invalid values
                for row in weight_matrix:
                    for val in row:
                        if not math.isfinite(val):
                            raise ValueError(f"Non-finite weight in {weight_name}: {val}")
            
            self.logger.info("‚úÖ Weights initialized successfully")
            return weights
            
        except Exception as e:
            self.logger.error(f"‚ùå Weight initialization failed: {e}")
            raise
    
    def create_robust_synthetic_data(self, num_nodes: int = 30, num_edges: int = 80) -> Dict[str, Any]:
        """Create synthetic data with robustness features."""
        try:
            with self.performance_monitor.monitor_operation("data_generation"):
                # Validate input parameters
                if num_nodes <= 0 or num_nodes > self.security_config.max_nodes:
                    raise ValidationError(f"Invalid num_nodes: {num_nodes}")
                if num_edges < 0 or num_edges > self.security_config.max_edges:
                    raise ValidationError(f"Invalid num_edges: {num_edges}")
                
                random.seed(42)
                
                # Generate node features with diversity
                node_features = []
                for i in range(num_nodes):
                    # Add structured patterns + noise for realism
                    base_pattern = [math.sin(i * 0.1 + j * 0.05) for j in range(self.node_dim // 2)]
                    noise_pattern = [random.gauss(0, 0.5) for _ in range(self.node_dim // 2)]
                    features = base_pattern + noise_pattern
                    
                    # Ensure proper length
                    while len(features) < self.node_dim:
                        features.append(random.gauss(0, 0.3))
                    features = features[:self.node_dim]
                    
                    node_features.append(features)
                
                # Generate edges with temporal patterns
                edges = []
                for _ in range(num_edges):
                    source = random.randint(0, num_nodes - 1)
                    target = random.randint(0, num_nodes - 1)
                    
                    if source != target:  # Avoid self-loops
                        # Create temporal clustering
                        cluster_time = random.uniform(0, 100)
                        temporal_noise = random.gauss(0, 5)
                        timestamp = max(0, cluster_time + temporal_noise)
                        
                        # Weight based on node distance (simulation of real networks)
                        distance_factor = abs(source - target) / num_nodes
                        base_weight = 1.0 - distance_factor * 0.5
                        weight = max(0.1, base_weight + random.gauss(0, 0.2))
                        
                        edges.append((source, target, timestamp, weight))
                
                # Sort by timestamp for temporal realism
                edges.sort(key=lambda x: x[2])
                
                data = {
                    'node_features': node_features,
                    'edges': edges,
                    'num_nodes': num_nodes,
                    'num_edges': len(edges)
                }
                
                # Validate generated data
                data = self.validator.validate_graph_data(data)
                
                self.logger.info(f"üéØ Generated robust synthetic data: {num_nodes} nodes, {len(edges)} edges")
                return data
                
        except Exception as e:
            self.logger.error(f"‚ùå Data generation failed: {e}")
            self.metrics.validation_errors += 1
            raise
    
    def robust_temporal_encoding(self, timestamps: List[float], dim: int = 32) -> List[List[float]]:
        """Robust temporal encoding with error handling."""
        try:
            if not timestamps:
                raise ValidationError("Empty timestamps list")
            
            # Check cache first
            cache_key = self.performance_monitor.get_cache_key("temporal_encoding", len(timestamps), dim)
            cached_result = self.performance_monitor.cache_get(cache_key)
            if cached_result is not None:
                self.metrics.cache_hits += 1
                return cached_result
            
            self.metrics.cache_misses += 1
            
            encoding = []
            
            # Robust frequency calculation
            max_timestamp = max(timestamps) if timestamps else 1.0
            normalization_factor = max(max_timestamp, 1.0)
            
            for t in timestamps:
                # Validate timestamp
                if not math.isfinite(t):
                    self.logger.warning(f"Invalid timestamp {t}, using 0.0")
                    t = 0.0
                
                # Normalize timestamp to prevent extreme values
                normalized_t = t / normalization_factor
                
                features = []
                for i in range(dim // 2):
                    try:
                        # Robust frequency calculation
                        freq = 1.0 / max(1.0, 10000.0 ** (2 * i / dim))
                        
                        # Safe trigonometric calculations
                        angle = normalized_t * freq
                        sin_val = math.sin(angle)
                        cos_val = math.cos(angle)
                        
                        # Validate outputs
                        if not (math.isfinite(sin_val) and math.isfinite(cos_val)):
                            sin_val, cos_val = 0.0, 1.0
                        
                        features.extend([sin_val, cos_val])
                        
                    except (OverflowError, ValueError) as e:
                        self.logger.warning(f"Temporal encoding error at frequency {i}: {e}")
                        features.extend([0.0, 1.0])  # Safe fallback
                
                # Ensure proper length
                while len(features) < dim:
                    features.append(0.0)
                features = features[:dim]
                
                encoding.append(features)
            
            # Cache result
            self.performance_monitor.cache_set(cache_key, encoding)
            
            return encoding
            
        except Exception as e:
            self.logger.error(f"‚ùå Temporal encoding failed: {e}")
            self.metrics.validation_errors += 1
            
            # Return safe fallback encoding
            return [[0.0] * dim for _ in range(len(timestamps))]
    
    def safe_matrix_multiply(self, a: List[List[float]], b: List[List[float]]) -> List[List[float]]:
        """Safe matrix multiplication with validation."""
        try:
            rows_a, cols_a = len(a), len(a[0]) if a else 0
            rows_b, cols_b = len(b), len(b[0]) if b else 0
            
            if cols_a != rows_b:
                raise ValidationError(f"Matrix dimension mismatch: {cols_a} != {rows_b}")
            
            if rows_a == 0 or cols_b == 0:
                raise ValidationError("Empty matrix dimensions")
            
            # Initialize result matrix
            result = [[0.0 for _ in range(cols_b)] for _ in range(rows_a)]
            
            # Perform multiplication with overflow protection
            for i in range(rows_a):
                for j in range(cols_b):
                    sum_val = 0.0
                    
                    for k in range(cols_a):
                        try:
                            product = a[i][k] * b[k][j]
                            
                            if math.isfinite(product):
                                sum_val += product
                            else:
                                self.logger.warning(f"Non-finite product at ({i},{j},{k})")
                                self.metrics.auto_corrections += 1
                                
                        except (OverflowError, TypeError):
                            self.logger.warning(f"Multiplication error at ({i},{j},{k})")
                            self.metrics.auto_corrections += 1
                    
                    # Clamp extreme values
                    if abs(sum_val) > 1e6:
                        sum_val = math.copysign(1e6, sum_val)
                        self.metrics.auto_corrections += 1
                    
                    result[i][j] = sum_val
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Matrix multiplication failed: {e}")
            self.metrics.validation_errors += 1
            
            # Return safe fallback
            rows_a = len(a) if a else 1
            cols_b = len(b[0]) if b and b[0] else 1
            return [[0.0 for _ in range(cols_b)] for _ in range(rows_a)]
    
    def robust_forward_pass(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Robust forward pass with comprehensive error handling."""
        self.status = DGDNStatus.PROCESSING
        
        try:
            with self.performance_monitor.monitor_operation("forward_pass"):
                # Input validation
                validated_data = self.validator.validate_graph_data(data)
                data_hash = self.validator.compute_data_hash(validated_data)
                
                # Extract validated data
                node_features = validated_data['node_features']
                edges = validated_data['edges']
                timestamps = [edge[2] for edge in edges]
                
                # Temporal encoding with error handling
                temporal_encoding = self.robust_temporal_encoding(timestamps)
                
                # Multi-layer processing
                current_embeddings = node_features
                layer_outputs = []
                
                for layer in range(self.num_layers):
                    try:
                        # Safe message passing
                        new_embeddings = self._safe_message_passing(
                            current_embeddings, edges, temporal_encoding
                        )
                        
                        layer_outputs.append(new_embeddings)
                        current_embeddings = new_embeddings
                        
                    except Exception as e:
                        self.logger.warning(f"Layer {layer} processing error: {e}")
                        # Fallback: use previous layer output
                        if layer_outputs:
                            current_embeddings = layer_outputs[-1]
                        else:
                            current_embeddings = node_features
                        self.metrics.auto_corrections += 1
                
                # Final embeddings
                final_embeddings = current_embeddings
                
                # Robust uncertainty quantification
                uncertainty_mean, uncertainty_std = self._robust_uncertainty_quantification(final_embeddings)
                
                # Output validation
                output = {
                    'node_embeddings': final_embeddings,
                    'uncertainty_mean': uncertainty_mean,
                    'uncertainty_std': uncertainty_std,
                    'temporal_encoding': temporal_encoding,
                    'layer_outputs': layer_outputs,
                    'data_hash': data_hash,
                    'status': 'success',
                    'robustness_metrics': {
                        'validation_errors': self.metrics.validation_errors,
                        'auto_corrections': self.metrics.auto_corrections,
                        'cache_hits': self.metrics.cache_hits
                    }
                }
                
                # Validate output
                if self.security_config.enable_output_validation:
                    self._validate_output(output)
                
                self.status = DGDNStatus.COMPLETED
                return output
                
        except Exception as e:
            self.status = DGDNStatus.FAILED
            self.logger.error(f"‚ùå Forward pass failed: {e}")
            self.logger.error(traceback.format_exc())
            
            # Return safe fallback output
            return self._create_fallback_output(data)
    
    def _safe_message_passing(self, node_features: List[List[float]], 
                             edges: List[Tuple], temporal_encoding: List[List[float]]) -> List[List[float]]:
        """Safe message passing with error recovery."""
        try:
            num_nodes = len(node_features)
            hidden_dim = self.hidden_dim
            
            # Initialize embeddings safely
            node_embeddings = [[0.0 for _ in range(hidden_dim)] for _ in range(num_nodes)]
            
            # Process edges with error handling
            for edge_idx, (source, target, timestamp, weight) in enumerate(edges):
                try:
                    if edge_idx >= len(temporal_encoding):
                        continue
                    
                    # Validate indices (should be caught by validator but double-check)
                    if not (0 <= source < num_nodes and 0 <= target < num_nodes):
                        self.metrics.validation_errors += 1
                        continue
                    
                    # Get source features safely
                    source_features = node_features[source]
                    
                    # Safe projection
                    projected = self._safe_linear_projection(
                        source_features, self.weights['node_projection']
                    )
                    
                    # Safe temporal projection
                    temporal_features = temporal_encoding[edge_idx]
                    temporal_projected = self._safe_linear_projection(
                        temporal_features, self.weights['temporal_projection']
                    )
                    
                    # Combine with weights
                    message = []
                    for i in range(hidden_dim):
                        try:
                            combined = (
                                projected[i] * self.adaptive_params['message_strength'] +
                                temporal_projected[i] * self.adaptive_params['temporal_weight']
                            ) * weight * self.adaptive_params['robustness_factor']
                            
                            # Clamp message values
                            if not math.isfinite(combined):
                                combined = 0.0
                                self.metrics.auto_corrections += 1
                            elif abs(combined) > 10.0:
                                combined = math.copysign(10.0, combined)
                                self.metrics.auto_corrections += 1
                            
                            message.append(combined)
                            
                        except (OverflowError, ValueError):
                            message.append(0.0)
                            self.metrics.auto_corrections += 1
                    
                    # Safe aggregation
                    for i in range(hidden_dim):
                        node_embeddings[target][i] += message[i]
                        
                except Exception as e:
                    self.logger.warning(f"Edge processing error at {edge_idx}: {e}")
                    self.metrics.validation_errors += 1
                    continue
            
            # Apply safe activation
            return self._safe_activation(node_embeddings)
            
        except Exception as e:
            self.logger.error(f"Message passing failed: {e}")
            self.metrics.validation_errors += 1
            
            # Fallback to identity
            return node_features if len(node_features[0]) == self.hidden_dim else [
                [0.0] * self.hidden_dim for _ in range(len(node_features))
            ]
    
    def _safe_linear_projection(self, features: List[float], weights: List[List[float]]) -> List[float]:
        """Safe linear projection with error handling."""
        try:
            if len(features) != len(weights):
                raise ValidationError(f"Feature-weight dimension mismatch: {len(features)} != {len(weights)}")
            
            output_dim = len(weights[0])
            result = [0.0] * output_dim
            
            for i in range(len(features)):
                for j in range(output_dim):
                    try:
                        product = features[i] * weights[i][j]
                        if math.isfinite(product):
                            result[j] += product
                        else:
                            self.metrics.auto_corrections += 1
                    except (OverflowError, TypeError):
                        self.metrics.auto_corrections += 1
            
            return result
            
        except Exception as e:
            self.logger.warning(f"Linear projection failed: {e}")
            return [0.0] * len(weights[0])
    
    def _safe_activation(self, x: List[List[float]], activation: str = 'relu') -> List[List[float]]:
        """Safe activation function with error handling."""
        result = []
        
        for row in x:
            new_row = []
            for val in row:
                try:
                    if not math.isfinite(val):
                        new_val = 0.0
                        self.metrics.auto_corrections += 1
                    elif activation == 'relu':
                        new_val = max(0.0, val)
                    elif activation == 'tanh':
                        new_val = math.tanh(max(-50, min(50, val)))  # Prevent overflow
                    elif activation == 'sigmoid':
                        new_val = 1.0 / (1.0 + math.exp(-max(-500, min(500, val))))
                    else:
                        new_val = val
                    
                    new_row.append(new_val)
                    
                except (OverflowError, ValueError):
                    new_row.append(0.0)
                    self.metrics.auto_corrections += 1
            
            result.append(new_row)
        
        return result
    
    def _robust_uncertainty_quantification(self, embeddings: List[List[float]]) -> Tuple[float, float]:
        """Robust uncertainty quantification with error handling."""
        try:
            all_values = []
            for row in embeddings:
                for val in row:
                    if math.isfinite(val):
                        all_values.append(val)
            
            if not all_values:
                return 0.0, 1.0  # Safe defaults
            
            # Robust statistics
            n = len(all_values)
            mean_val = sum(all_values) / n
            
            # Use median absolute deviation for robustness
            abs_deviations = [abs(x - mean_val) for x in all_values]
            abs_deviations.sort()
            
            if n % 2 == 0:
                mad = (abs_deviations[n//2-1] + abs_deviations[n//2]) / 2
            else:
                mad = abs_deviations[n//2]
            
            # Convert MAD to standard deviation estimate
            std_estimate = mad * 1.4826  # For normal distribution
            
            # Apply adaptive scaling
            scaled_uncertainty = std_estimate * self.adaptive_params['uncertainty_scale']
            
            return float(mean_val), float(scaled_uncertainty)
            
        except Exception as e:
            self.logger.warning(f"Uncertainty quantification failed: {e}")
            return 0.0, 1.0
    
    def _validate_output(self, output: Dict[str, Any]) -> None:
        """Validate model output for security and correctness."""
        try:
            required_keys = ['node_embeddings', 'uncertainty_mean', 'uncertainty_std']
            for key in required_keys:
                if key not in output:
                    raise ValidationError(f"Missing output key: {key}")
            
            # Validate embeddings
            embeddings = output['node_embeddings']
            if not embeddings or not embeddings[0]:
                raise ValidationError("Empty embeddings")
            
            # Check for malicious patterns
            for i, row in enumerate(embeddings):
                for j, val in enumerate(row):
                    if not math.isfinite(val):
                        raise ValidationError(f"Non-finite embedding at [{i}][{j}]: {val}")
                    if abs(val) > 1000:
                        self.metrics.security_warnings += 1
                        self.logger.warning(f"Large embedding value at [{i}][{j}]: {val}")
            
            self.logger.info("‚úÖ Output validation passed")
            
        except Exception as e:
            self.logger.error(f"‚ùå Output validation failed: {e}")
            raise
    
    def _create_fallback_output(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Create safe fallback output in case of complete failure."""
        num_nodes = data.get('num_nodes', 1)
        
        return {
            'node_embeddings': [[0.0] * self.hidden_dim for _ in range(num_nodes)],
            'uncertainty_mean': 0.0,
            'uncertainty_std': 1.0,
            'temporal_encoding': [],
            'layer_outputs': [],
            'data_hash': 'fallback',
            'status': 'fallback',
            'robustness_metrics': {
                'validation_errors': self.metrics.validation_errors,
                'auto_corrections': self.metrics.auto_corrections,
                'fallback_used': True
            }
        }
    
    def autonomous_robustness_optimization(self, metrics: Dict[str, float], iteration: int) -> None:
        """Autonomous optimization focused on robustness."""
        try:
            # Robustness-focused adaptations
            if self.metrics.validation_errors > 0:
                self.adaptive_params['robustness_factor'] = min(2.0, self.adaptive_params['robustness_factor'] * 1.1)
                self.logger.info(f"   ‚Üí Increased robustness factor to {self.adaptive_params['robustness_factor']:.3f}")
            
            if self.metrics.auto_corrections > iteration * 2:  # Too many corrections
                self.adaptive_params['message_strength'] *= 0.95
                self.logger.info(f"   ‚Üí Reduced message strength to {self.adaptive_params['message_strength']:.3f}")
            
            # Performance-based optimizations
            if metrics.get('embedding_std', 0) < 0.1:
                self.adaptive_params['temporal_weight'] = min(1.0, self.adaptive_params['temporal_weight'] * 1.02)
                self.logger.info(f"   ‚Üí Increased temporal weight to {self.adaptive_params['temporal_weight']:.3f}")
            
            # Reset error counters periodically
            if iteration % 3 == 0:
                self.metrics.validation_errors = 0
                self.metrics.auto_corrections = 0
        
        except Exception as e:
            self.logger.warning(f"Robustness optimization failed: {e}")
    
    def run_robust_demonstration(self) -> Dict[str, Any]:
        """Run comprehensive robust demonstration."""
        self.logger.info("üõ°Ô∏è  Starting Robust DGDN Demonstration")
        self.logger.info("üìã Generation 2: Make It Robust - Comprehensive Error Handling")
        
        results = {
            'generation': 2,
            'implementation': 'robust',
            'status': 'running',
            'metrics_history': [],
            'robustness_report': {},
            'security_events': [],
            'performance_events': []
        }
        
        try:
            # Generate test data
            self.logger.info("üéØ Generating robust test data...")
            data = self.create_robust_synthetic_data(num_nodes=40, num_edges=120)
            
            # Run multiple iterations with error injection
            for iteration in range(7):
                self.logger.info(f"üîÑ Robustness Iteration {iteration + 1}/7")
                
                start_time = time.time()
                
                # Inject controlled errors for testing (except first iteration)
                if iteration > 0 and iteration % 3 == 0:
                    self.logger.info("   üß™ Injecting test errors for robustness validation...")
                    data = self._inject_test_errors(data, error_rate=0.05)
                
                # Execute with circuit breaker protection
                try:
                    output = self.circuit_breaker.call(self.robust_forward_pass, data)
                except PerformanceError as e:
                    self.logger.warning(f"Circuit breaker activated: {e}")
                    self.metrics.circuit_breaker_trips += 1
                    continue
                
                # Compute comprehensive metrics
                metrics = self._compute_comprehensive_metrics(output, data)
                metrics['iteration'] = iteration + 1
                metrics['inference_time'] = time.time() - start_time
                metrics['circuit_breaker_trips'] = self.metrics.circuit_breaker_trips
                
                results['metrics_history'].append(metrics)
                
                # Autonomous robustness optimization
                self.autonomous_robustness_optimization(metrics, iteration)
                
                # Log comprehensive status
                self.logger.info(f"   Status: {output['status']}")
                self.logger.info(f"   Embedding quality: {metrics['embedding_norm']:.3f}")
                self.logger.info(f"   Robustness score: {metrics['robustness_score']:.3f}")
                self.logger.info(f"   Auto-corrections: {metrics['auto_corrections']}")
                self.logger.info(f"   Cache efficiency: {metrics['cache_hit_rate']:.1%}")
            
            # Compile robustness report
            results['robustness_report'] = self._compile_robustness_report()
            results['status'] = 'completed'
            
            self.logger.info("‚úÖ Robust demonstration completed successfully")
            
        except Exception as e:
            self.logger.error(f"‚ùå Robust demonstration failed: {e}")
            results['status'] = 'failed'
            results['error'] = str(e)
        
        return results
    
    def _inject_test_errors(self, data: Dict[str, Any], error_rate: float = 0.05) -> Dict[str, Any]:
        """Inject controlled errors for robustness testing."""
        import copy
        test_data = copy.deepcopy(data)
        
        # Inject errors in node features
        num_errors = int(len(test_data['node_features']) * error_rate)
        for _ in range(num_errors):
            node_idx = random.randint(0, len(test_data['node_features']) - 1)
            feat_idx = random.randint(0, len(test_data['node_features'][node_idx]) - 1)
            
            # Various error types
            error_type = random.choice(['inf', 'nan', 'extreme'])
            if error_type == 'inf':
                test_data['node_features'][node_idx][feat_idx] = float('inf')
            elif error_type == 'nan':
                test_data['node_features'][node_idx][feat_idx] = float('nan')
            else:  # extreme
                test_data['node_features'][node_idx][feat_idx] = random.choice([1e10, -1e10])
        
        return test_data
    
    def _compute_comprehensive_metrics(self, output: Dict[str, Any], data: Dict[str, Any]) -> Dict[str, float]:
        """Compute comprehensive robustness and performance metrics."""
        metrics = {}
        
        try:
            # Basic embedding metrics
            embeddings = output['node_embeddings']
            if NUMPY_AVAILABLE:
                emb_array = np.array(embeddings)
                metrics['embedding_norm'] = float(np.linalg.norm(emb_array))
                metrics['embedding_mean'] = float(np.mean(emb_array))
                metrics['embedding_std'] = float(np.std(emb_array))
            else:
                flat_values = [val for row in embeddings for val in row]
                metrics['embedding_norm'] = math.sqrt(sum(x*x for x in flat_values))
                metrics['embedding_mean'] = sum(flat_values) / len(flat_values)
                mean_val = metrics['embedding_mean']
                metrics['embedding_std'] = math.sqrt(sum((x - mean_val)**2 for x in flat_values) / len(flat_values))
            
            # Robustness metrics
            robustness_metrics = output.get('robustness_metrics', {})
            metrics['validation_errors'] = robustness_metrics.get('validation_errors', 0)
            metrics['auto_corrections'] = robustness_metrics.get('auto_corrections', 0)
            metrics['cache_hits'] = self.metrics.cache_hits
            metrics['cache_misses'] = self.metrics.cache_misses
            
            # Calculate derived metrics
            total_cache_requests = metrics['cache_hits'] + metrics['cache_misses']
            metrics['cache_hit_rate'] = (metrics['cache_hits'] / max(total_cache_requests, 1))
            
            # Robustness score (higher is better)
            error_penalty = metrics['validation_errors'] * 0.1
            correction_penalty = metrics['auto_corrections'] * 0.05
            cache_bonus = metrics['cache_hit_rate'] * 0.1
            
            metrics['robustness_score'] = max(0, 1.0 - error_penalty - correction_penalty + cache_bonus)
            
            # Security metrics
            metrics['security_warnings'] = self.metrics.security_warnings
            
            # Add uncertainty metrics
            metrics['uncertainty_mean'] = output.get('uncertainty_mean', 0.0)
            metrics['uncertainty_std'] = output.get('uncertainty_std', 1.0)
            
            return metrics
            
        except Exception as e:
            self.logger.warning(f"Metrics computation failed: {e}")
            return {'robustness_score': 0.0, 'status': 'metrics_failed'}
    
    def _compile_robustness_report(self) -> Dict[str, Any]:
        """Compile comprehensive robustness report."""
        return {
            'total_validation_errors': self.metrics.validation_errors,
            'total_security_warnings': self.metrics.security_warnings,
            'total_performance_warnings': self.metrics.performance_warnings,
            'total_auto_corrections': self.metrics.auto_corrections,
            'circuit_breaker_trips': self.metrics.circuit_breaker_trips,
            'cache_performance': {
                'hits': self.metrics.cache_hits,
                'misses': self.metrics.cache_misses,
                'hit_rate': self.metrics.cache_hits / max(self.metrics.cache_hits + self.metrics.cache_misses, 1)
            },
            'adaptive_parameters': self.adaptive_params.copy(),
            'status': self.status.value,
            'features_implemented': [
                'Input validation and sanitization',
                'Circuit breaker pattern',
                'Performance monitoring',
                'Caching system',
                'Automatic error correction',
                'Security validation',
                'Comprehensive logging',
                'Autonomous optimization'
            ]
        }

def main():
    """Main execution function for robust DGDN demonstration."""
    logger.info("üõ°Ô∏è  Terragon Labs - Robust Autonomous DGDN")
    logger.info("üèóÔ∏è  Generation 2: Make It Robust")
    logger.info("="*80)
    
    # Configure enhanced settings
    security_config = SecurityConfig(
        max_nodes=1000,
        max_edges=10000,
        enable_input_sanitization=True,
        enable_output_validation=True
    )
    
    performance_config = PerformanceConfig(
        max_inference_time_ms=2000,
        enable_profiling=True,
        enable_caching=True
    )
    
    # Initialize robust DGDN
    robust_dgdn = RobustDGDN(
        node_dim=48,
        hidden_dim=96,
        num_layers=3,
        security_config=security_config,
        performance_config=performance_config
    )
    
    # Run comprehensive demonstration
    start_time = time.time()
    results = robust_dgdn.run_robust_demonstration()
    total_time = time.time() - start_time
    
    # Comprehensive reporting
    logger.info("\n" + "="*80)
    logger.info("üìä GENERATION 2 ROBUSTNESS REPORT")
    logger.info("="*80)
    
    logger.info(f"Status: {results['status'].upper()}")
    logger.info(f"Implementation: {results['implementation']}")
    logger.info(f"Total execution time: {total_time:.2f}s")
    
    if results['status'] == 'completed':
        # Performance summary
        final_metrics = results['metrics_history'][-1]
        logger.info(f"\nüéØ Final Performance:")
        logger.info(f"  ‚Ä¢ Robustness score: {final_metrics['robustness_score']:.3f}/1.0")
        logger.info(f"  ‚Ä¢ Embedding quality: {final_metrics['embedding_norm']:.3f}")
        logger.info(f"  ‚Ä¢ Cache hit rate: {final_metrics['cache_hit_rate']:.1%}")
        
        # Robustness analysis
        robustness_report = results['robustness_report']
        logger.info(f"\nüõ°Ô∏è  Robustness Analysis:")
        logger.info(f"  ‚Ä¢ Validation errors handled: {robustness_report['total_validation_errors']}")
        logger.info(f"  ‚Ä¢ Auto-corrections applied: {robustness_report['total_auto_corrections']}")
        logger.info(f"  ‚Ä¢ Security warnings: {robustness_report['total_security_warnings']}")
        logger.info(f"  ‚Ä¢ Circuit breaker trips: {robustness_report['circuit_breaker_trips']}")
        
        # Feature summary
        logger.info(f"\n‚ú® Robustness Features Implemented:")
        for feature in robustness_report['features_implemented']:
            logger.info(f"  ‚úì {feature}")
    
    logger.info(f"\nüöÄ Ready to proceed to Generation 3: Make It Scale")
    logger.info("="*80)
    
    return results

if __name__ == "__main__":
    results = main()
    exit_code = 0 if results.get('status') == 'completed' else 1
    sys.exit(exit_code)