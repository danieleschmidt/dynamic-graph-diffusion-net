# Performance Benchmarking Workflow Template
# Copy this file to .github/workflows/benchmark.yml

name: üèÉ Performance Benchmarks

on:
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_suite:
        description: 'Benchmark suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'training'
          - 'inference'
          - 'memory'
          - 'scalability'
      compare_with:
        description: 'Compare with (commit hash, tag, or branch)'
        required: false
        default: 'main'
        type: string
  issue_comment:
    types: [created]

env:
  PYTHON_VERSION: '3.9'
  BENCHMARK_RESULTS_DIR: 'benchmark-results'
  
jobs:
  # Only run on benchmark command in PR comments
  check-comment:
    name: üîç Check Comment Trigger
    runs-on: ubuntu-latest
    if: github.event_name == 'issue_comment'
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      benchmark_suite: ${{ steps.check.outputs.benchmark_suite }}
    
    steps:
      - name: Check comment for benchmark command
        id: check
        run: |
          COMMENT="${{ github.event.comment.body }}"
          if [[ "$COMMENT" =~ ^/benchmark(\s+(.+))?$ ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            SUITE="${BASH_REMATCH[2]:-all}"
            echo "benchmark_suite=${SUITE}" >> $GITHUB_OUTPUT
            echo "‚úÖ Benchmark command detected: /benchmark ${SUITE}"
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è No benchmark command found in comment"
          fi

  setup-matrix:
    name: üìã Setup Benchmark Matrix
    runs-on: ubuntu-latest
    if: github.event_name != 'issue_comment' || needs.check-comment.outputs.should_run == 'true'
    needs: [check-comment]
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
      benchmark_suite: ${{ steps.suite.outputs.suite }}
    
    steps:
      - name: Determine benchmark suite
        id: suite
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            SUITE="${{ github.event.inputs.benchmark_suite }}"
          elif [[ "${{ github.event_name }}" == "issue_comment" ]]; then
            SUITE="${{ needs.check-comment.outputs.benchmark_suite }}"
          else
            SUITE="all"
          fi
          echo "suite=${SUITE}" >> $GITHUB_OUTPUT
          echo "Selected benchmark suite: ${SUITE}"

      - name: Create benchmark matrix
        id: matrix
        run: |
          case "${{ steps.suite.outputs.suite }}" in
            "training")
              MATRIX='["training_performance", "training_memory"]'
              ;;
            "inference")
              MATRIX='["inference_latency", "inference_throughput"]'
              ;;
            "memory")
              MATRIX='["memory_usage", "memory_efficiency"]'
              ;;
            "scalability")
              MATRIX='["batch_scaling", "model_scaling"]'
              ;;
            "all"|*)
              MATRIX='["training_performance", "training_memory", "inference_latency", "inference_throughput", "memory_usage", "memory_efficiency", "batch_scaling", "model_scaling"]'
              ;;
          esac
          echo "matrix=${MATRIX}" >> $GITHUB_OUTPUT
          echo "Benchmark matrix: ${MATRIX}"

  run-benchmarks:
    name: üèÉ Run ${{ matrix.benchmark }}
    runs-on: ubuntu-latest
    needs: [setup-matrix]
    if: github.event_name != 'issue_comment' || needs.check-comment.outputs.should_run == 'true'
    strategy:
      fail-fast: false
      matrix:
        benchmark: ${{ fromJson(needs.setup-matrix.outputs.matrix) }}
        python-version: ['3.9', '3.10']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[benchmark]"
          pip install pytest-benchmark matplotlib pandas

      - name: Setup benchmark environment
        run: |
          mkdir -p ${{ env.BENCHMARK_RESULTS_DIR }}
          echo "BENCHMARK_OUTPUT_DIR=${{ env.BENCHMARK_RESULTS_DIR }}" >> $GITHUB_ENV
          
          # Set CPU affinity for consistent results
          echo "PYTHONHASHSEED=42" >> $GITHUB_ENV
          echo "OMP_NUM_THREADS=1" >> $GITHUB_ENV

      - name: Run ${{ matrix.benchmark }} benchmark
        run: |
          echo "üèÉ Running ${{ matrix.benchmark }} benchmark..."
          
          case "${{ matrix.benchmark }}" in
            "training_performance")
              python -m pytest benchmarks/test_training_performance.py \
                --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/training_performance_py${{ matrix.python-version }}.json \
                --benchmark-warmup=3 --benchmark-min-rounds=5
              ;;
            "training_memory")
              python -m pytest benchmarks/test_training_memory.py \
                --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/training_memory_py${{ matrix.python-version }}.json \
                --benchmark-warmup=2 --benchmark-min-rounds=3
              ;;
            "inference_latency")
              python -m pytest benchmarks/test_inference_latency.py \
                --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/inference_latency_py${{ matrix.python-version }}.json \
                --benchmark-warmup=5 --benchmark-min-rounds=10
              ;;
            "inference_throughput")
              python -m pytest benchmarks/test_inference_throughput.py \
                --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/inference_throughput_py${{ matrix.python-version }}.json \
                --benchmark-warmup=3 --benchmark-min-rounds=5
              ;;
            "memory_usage")
              python -m pytest benchmarks/test_memory_usage.py \
                --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/memory_usage_py${{ matrix.python-version }}.json \
                --benchmark-warmup=2 --benchmark-min-rounds=3
              ;;
            "memory_efficiency")
              python -m pytest benchmarks/test_memory_efficiency.py \
                --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/memory_efficiency_py${{ matrix.python-version }}.json \
                --benchmark-warmup=2 --benchmark-min-rounds=3
              ;;
            "batch_scaling")
              python -m pytest benchmarks/test_batch_scaling.py \
                --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/batch_scaling_py${{ matrix.python-version }}.json \
                --benchmark-warmup=2 --benchmark-min-rounds=3
              ;;
            "model_scaling")
              python -m pytest benchmarks/test_model_scaling.py \
                --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/model_scaling_py${{ matrix.python-version }}.json \
                --benchmark-warmup=2 --benchmark-min-rounds=3
              ;;
          esac

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results-${{ matrix.benchmark }}-py${{ matrix.python-version }}
          path: ${{ env.BENCHMARK_RESULTS_DIR }}/*.json

      - name: Generate benchmark summary
        run: |
          python scripts/generate_benchmark_summary.py \
            --input-dir ${{ env.BENCHMARK_OUTPUT_DIR }} \
            --benchmark ${{ matrix.benchmark }} \
            --python-version ${{ matrix.python-version }} \
            --output ${{ env.BENCHMARK_OUTPUT_DIR }}/summary_${{ matrix.benchmark }}_py${{ matrix.python-version }}.md

      - name: Upload benchmark summary
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-summary-${{ matrix.benchmark }}-py${{ matrix.python-version }}
          path: ${{ env.BENCHMARK_RESULTS_DIR }}/summary_*.md

  compare-benchmarks:
    name: üìä Compare Benchmarks
    runs-on: ubuntu-latest
    needs: [setup-matrix, run-benchmarks]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.compare_with != ''
    
    steps:
      - name: Checkout current code
        uses: actions/checkout@v4
        with:
          path: current

      - name: Checkout comparison code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.compare_with }}
          path: comparison

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download benchmark results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-artifacts

      - name: Install comparison tools
        run: |
          pip install pandas matplotlib seaborn
          
      - name: Run benchmark comparison
        run: |
          cd current
          python scripts/compare_benchmarks.py \
            --current-dir ../benchmark-artifacts \
            --comparison-ref ${{ github.event.inputs.compare_with }} \
            --output ../benchmark-comparison.md

      - name: Upload comparison results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-comparison
          path: benchmark-comparison.md

  generate-report:
    name: üìà Generate Benchmark Report
    runs-on: ubuntu-latest
    needs: [setup-matrix, run-benchmarks]
    if: always() && needs.run-benchmarks.result != 'cancelled'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download all benchmark results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-artifacts

      - name: Install report generation tools
        run: |
          pip install pandas matplotlib seaborn jinja2

      - name: Generate comprehensive report
        run: |
          python scripts/generate_benchmark_report.py \
            --input-dir benchmark-artifacts \
            --output-dir benchmark-report \
            --format html \
            --include-charts

      - name: Upload benchmark report
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-report
          path: benchmark-report/

      - name: Generate performance badge
        run: |
          python scripts/generate_performance_badge.py \
            --input-dir benchmark-artifacts \
            --output benchmark-report/performance-badge.svg

  update-performance-tracking:
    name: üìä Update Performance Tracking
    runs-on: ubuntu-latest
    needs: [run-benchmarks, generate-report]
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download benchmark results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-artifacts

      - name: Update performance history
        run: |
          # Create performance tracking directory if it doesn't exist
          mkdir -p .github/performance-history
          
          # Archive current results
          DATE=$(date +%Y-%m-%d)
          COMMIT=$(git rev-parse --short HEAD)
          mkdir -p .github/performance-history/${DATE}-${COMMIT}
          cp -r benchmark-artifacts/* .github/performance-history/${DATE}-${COMMIT}/

      - name: Commit performance history
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .github/performance-history
          git commit -m "üìä Update performance history for $(date +%Y-%m-%d)" || echo "No changes to commit"
          git push

  comment-results:
    name: üí¨ Comment Results
    runs-on: ubuntu-latest
    needs: [check-comment, setup-matrix, run-benchmarks, generate-report]
    if: github.event_name == 'issue_comment' && needs.check-comment.outputs.should_run == 'true'
    
    steps:
      - name: Download benchmark report
        uses: actions/download-artifact@v3
        with:
          name: benchmark-report
          path: benchmark-report

      - name: Create comment body
        id: comment
        run: |
          echo "comment<<EOF" >> $GITHUB_OUTPUT
          echo "## üèÉ Benchmark Results" >> $GITHUB_OUTPUT
          echo "" >> $GITHUB_OUTPUT
          echo "**Benchmark Suite:** \`${{ needs.setup-matrix.outputs.benchmark_suite }}\`" >> $GITHUB_OUTPUT
          echo "**Triggered by:** @${{ github.event.comment.user.login }}" >> $GITHUB_OUTPUT
          echo "**Commit:** \`${{ github.sha }}\`" >> $GITHUB_OUTPUT
          echo "" >> $GITHUB_OUTPUT
          
          if [[ -f benchmark-report/summary.md ]]; then
            cat benchmark-report/summary.md >> $GITHUB_OUTPUT
          else
            echo "Benchmark results are being processed. Check the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed results." >> $GITHUB_OUTPUT
          fi
          
          echo "" >> $GITHUB_OUTPUT
          echo "---" >> $GITHUB_OUTPUT
          echo "ü§ñ *Automated benchmark results*" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Comment on PR
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${{ steps.comment.outputs.comment }}`
            })

  alert-regression:
    name: üö® Alert Performance Regression
    runs-on: ubuntu-latest
    needs: [run-benchmarks, generate-report]
    if: always() && needs.run-benchmarks.result == 'success'
    
    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-artifacts

      - name: Check for performance regressions
        id: regression
        run: |
          # This would typically compare against baseline performance
          # For now, we'll create a placeholder check
          echo "üîç Checking for performance regressions..."
          
          # Example regression detection logic
          REGRESSION_FOUND=false
          
          # Set output for conditional steps
          echo "regression_found=${REGRESSION_FOUND}" >> $GITHUB_OUTPUT

      - name: Create regression issue
        if: steps.regression.outputs.regression_found == 'true'
        uses: actions/github-script@v6
        with:
          script: |
            const title = `üö® Performance Regression Detected - ${new Date().toISOString().split('T')[0]}`;
            const body = `
            ## Performance Regression Alert
            
            A performance regression has been detected in the latest benchmark run.
            
            **Details:**
            - Commit: \`${{ github.sha }}\`
            - Workflow: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            - Date: ${new Date().toISOString()}
            
            Please review the benchmark results and investigate the cause of the regression.
            
            ## Next Steps
            1. Review the benchmark report artifacts
            2. Compare with previous baseline performance
            3. Identify the root cause of the regression
            4. Implement performance improvements
            
            ---
            ü§ñ *Automated performance regression detection*
            `;
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['performance', 'regression', 'bug']
            });

  cleanup:
    name: üßπ Cleanup
    runs-on: ubuntu-latest
    needs: [run-benchmarks, generate-report, comment-results, update-performance-tracking]
    if: always()
    
    steps:
      - name: Cleanup temporary files
        run: |
          echo "üßπ Cleaning up temporary benchmark files..."
          # Add any cleanup logic here
          echo "‚úÖ Cleanup completed"